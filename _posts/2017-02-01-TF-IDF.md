---
title:  "Giải thích và code Term Frequency - Inverse Document Frequency"
mathjax: true
layout: post
categories: media
---

Xin chào các bạn,

Trong bài post này, mình sẽ giới thiệu một phương pháp để xác định độ quan trọng của từng từ trong một câu. 

![Image](https://www.kdnuggets.com/wp-content/uploads/arya-tf-idf-defined-0-1024x573.png)

Vậy mức độ quan trọng của một từ là gì? Mức độ quan trọng ở đây là một con số cụ thể  nào đó và nếu nó lớn thì tức là tư đó quan trọng và ngược lại. Giả sử, bài phát biểu nhậm chức của tổng thống Mỹ như sau: "_Tôi sẽ tập trung vào y tế_", thì trong câu sau từ "_y tế_" nên có mức độ quan trọng lớn hơn các từ như "_Tôi_", "_sẽ_", "_tập_", "_trung_", "_vào_". 

### 1. Giới thiệu TF-IDF
Như tiêu đề thì TF-IDF là từ viết tắt của cụm Term Frequency - Inverse Document Frequency. Giả thuật này được hai nhà khpa học máy tính Hans Peter Luhn và Karen Spärck Jones tìm ra. Cụ thể hơn, Hans là người phát triển phần term frequency và Karen là người thêm phần Inverse Document Frequency vào giải thuật. 

### 2. Giải thuật 

Trước khi đi vào công thức, mình muốn làm rõ các terminologies trước. 

* Documents (D): Là tập hợp tất cả những document. Ví dụ [["Tôi", "tên", "A"], ["Tôi", "là", "B"]]

* N: Tổng số lượng các documents. Ví dụ, N = 2 trong trường hợp này.

* Term (t): Từ. Ví dụ "Tôi, "tên, ...


Để tính TF-IDF cho một từ trong một document, ta tính như sau: 

$$
TF(term, document) = \frac{f_{t, d}}{\sum_{t' \in d} f_{t', d}}
$$

$$
IDF(term, Documents) = \log \frac{N}{| \{ d \in D : t \in d \} |}
$$

$$TF-IDF (term, document, Documents) = TF(term, document) \times IDF(term, Documents)$$

Lấy một ví dụ đơn giản là tính TF-IDF của từ "Tôi" trong document ["Tôi", "tên", "A"] trong tập documents [["Tôi", "tên", "A"], ["Tôi", "là", "B"]]. Ta tính lần lượt như sau: 

Với $TF(term, document)$, ta đơn giản là tính tổng số lần xuất hiện của term trong document đó. Ví dụ ["Tôi", "tên", "A"] thì $TF("Tôi", document) = \frac{1}{3}$. Đối với $IDF(term, Documents)$, ta đơn giản là lấy tổng số lượng documents chia cho số lượng documents có chứa từ đó, ở case này là $ IDF("Tôi", Documents) = \log\frac{2}{2} = 0$.  

Vì vậy, ta có TF-IDF của từ "Tôi" trong document ["Tôi", "tên", "A"] trong tập documents [["Tôi", "tên", "A"], ["Tôi", "là", "B"]] là $\frac{1}{3} \times \log(\frac{2}{2}) = 0$.

Ở case này, TF-IDF của từ này rất nhỏ và đáng lẽ ra không nên bằng 0, các bạn có thể giải quyết vấn đề này bằng cách offset một $\epsilon$ cho cả numerator và denominator của IDF. Tuy nhiên, kết quả trên phản ánh đúng công dụng của TF-IDF với việc các từ xuất hiện nhiều và hầu như ở tất cả các văn bản như "anh", "thì", "là" sẽ có TF-IDF cực kì thấp do chúng không phải là những từ quan trọng. 


* **Tại sao dùng Inverse Document Frequency (IDF) thay vì Document Frequency (DF)?**

Document Frequency (DF) đo lường mức độ phổ biến của một từ, được tính là số lượng tài liệu chứa từ đó. Tuy nhiên, chỉ sử dụng DF có vấn đề:

Thiếu sự phân biệt giữa từ phổ biến và không phổ biến:
Các từ phổ biến như "the", "and", hoặc "is" xuất hiện trong hầu hết các tài liệu và có giá trị DF cao. Nhưng những từ này không mang ý nghĩa phân biệt khi so sánh giữa các tài liệu.
Inverse Document Frequency (IDF) được phát minh để giảm trọng số của các từ phổ biến và tăng trọng số cho từ hiếm. Cụ thể:

Lợi ích của việc lấy nghịch đảo:
Khi một từ xuất hiện trong nhiều tài liệu, giá trị sẽ rất nhỏ, làm giảm tầm quan trọng của từ đó. Ngược lại, từ ít xuất hiện có nhỏ, làm tăng giá trị IDF.

Ngoải ra, hàm logarit giúp điều chỉnh giá trị IDF để không quá chênh lệch giữa các từ hiếm và phổ biến. Ví dụ, nếu không có log, từ xuất hiện trong 1 tài liệu có IDF lớn gấp hàng nghìn lần từ xuất hiện trong 2 tài liệu, gây mất cân bằng.

* **Tại sao phải nhân TF và IDF?**

Nhược điểm khi dùng TF hoặc IDF riêng lẻ:

TF (Term Frequency):
Chỉ dựa vào TF sẽ ưu tiên các từ xuất hiện nhiều lần trong document mà không xét đến tính phổ biến của từ trong toàn bộ documents. Điều này dẫn đến việc từ thông dụng (như "the") có trọng số cao trong mọi documents.

IDF (Inverse Document Frequency):
Chỉ dùng IDF mà không xét đến TF sẽ gây mất thông tin về tần suất từ xuất hiện trong document cụ thể. Ví dụ, nếu một từ hiếm xuất hiện chỉ một lần trong documents, IDF cao nhưng tầm quan trọng thực sự của từ đó trong document cụ thể là thấp.

Vì vậy, việc nhân TF và IDF tạo ra một chỉ số cân bằng, cho phép các từ mang tính đặc trưng cao của tài liệu nổi bật hơn.


### 3. Ứng dụng của TF-IDF 

* **Hệ thống tìm kiếm và xếp hạng tài liệu** 

Sử dụng TF-IDF để trích xuất ra các từ khoá của nội dung tìm kiếm và từ đó trả về các nội dung có giá trị liên quan cao. Giả sử như bạn nhập một đoạn text hỏi về chiến tranh thế giới thứ 2, diễn ra năm bao nhiêu, ai tấn công trước, ... Giải thuật TF-IDF sẽ tìm ra các từ khoá là "chiến" "tranh" "thế" "giới" "thứ" "2" và từ đó trả về các kết quả liên quan. 

* **Tóm tắt văn bản tự động** 

Dùng TF-IDF để trích xuất ra các từ khoá ngắn gọn bao trọn hàm ý của văn bản. Giả sử, bộ phận tuyển dụng gửi một email dài nhưng bạn chỉ cần xem liệu có từ "regret", "sorry" trong đó không là đủ. Với trường hợp này, ta chỉ cần sử dụng TF-IDF để xem các từ có trọng số cao có các từ trên không thay vì đọc hết email. 

* **Phân loại văn bản** 

Dùng TF-IDF để trích xuất ra tất cả các từ trong một document cụ thể và dùng các chỉ số này như một feature để train SVM, Decision Tree, ...

* **Gợi ý sản phẩm (Recommendation)**

Dùng TF-IDF để trích xuất ra các từ khoá (có TF-IDF) cao từ review mua hàng của khách hàng, e.g. "bền", "rẻ", "đỉnh", ..., để gợi ý các sản phẩm có đặc tính tương tự. 

### 4. Điểm yếu của TF-IDF 

* **Không xử lý được ngữ nghĩa** 

* * TF-IDF chỉ dựa vào tần suất của từ trong tài liệu và tập dữ liệu, không hiểu được ngữ nghĩa hoặc mối quan hệ giữa các từ.


* * Ví dụ: Các từ như "đẹp" và "xinh" có ý nghĩa tương đương nhưng TF-IDF lại xử lý chúng như hai từ hoàn toàn khác nhau.

* **Không xử lý được ngữ cảnh**

* * TF-IDF không thể nắm bắt ngữ cảnh của từ trong câu. Một từ có thể mang ý nghĩa khác nhau tùy thuộc vào ngữ cảnh, nhưng TF-IDF không phân biệt được.

* * Ví dụ: Từ "bank" có thể chỉ "ngân hàng" hoặc "bờ sông" tùy vào câu, nhưng TF-IDF coi nó như một từ duy nhất.

* **Không tối ưu trên dữ liệu lớn** 

* * Với tập dữ liệu khổng lồ, ma trận TF-IDF trở nên rất lớn và thưa, gây tốn bộ nhớ và làm giảm hiệu quả tính toán.

* **Nhạy cảm với từ thông dụng** 

* * TF-IDF cố gắng giảm trọng số của các từ phổ biến, nhưng trong thực tế, một số từ phổ biến có thể mang ý nghĩa quan trọng trong bối cảnh nhất định, và TF-IDF lại bỏ qua điều này.

* * Ví dụ: Trong một bài phát biểu, các từ như "tôi", "chúng ta" có thể rất quan trọng.

* **Không động và không linh hoạt**

* * TF-IDF không thích nghi tốt khi dữ liệu liên tục thay đổi. Mỗi khi thêm hoặc xóa tài liệu, toàn bộ ma trận TF-IDF phải được tính toán lại.

### 5. Tổng kết 


Trong bài viết này, mình đã trình bày khái niệm TF-IDF, cách nó hoạt động cũng như các ứng dụng thực tiễn. TF-IDF là một phương pháp đơn giản, dễ triển khai và rất phù hợp cho các bài toán xử lý văn bản không yêu cầu hiểu sâu về ngữ nghĩa.

Tuy nhiên, đối với những bài toán phức tạp hoặc cần xử lý khối lượng dữ liệu lớn, bạn nên cân nhắc sử dụng các phương pháp hiện đại như transformers hoặc word embeddings để đạt hiệu quả tốt hơn.

### References
1\. [TF-IDF — Term Frequency-Inverse Document Frequency][fatih_blog]  
2\. [Term Frequency Inverse Document Frequency (TF-IDF) Explained - Youtube][Youtube]  


[fatih_blog]: https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency/ 
[Youtube]: https://www.youtube.com/watch?v=zLMEnNbdh4Q