---
title: 'Explain Non-Maximum Suppression and build it from scratch'
date: 2024-03-23
permalink: /posts/2024/03/explain-nms-and-code-from-scratch/
tags:
  - Object Detection
  - Post-processing
---


Xin ch√†o c√°c b·∫°n! Trong b√†i vi·∫øt n√†y, m√¨nh s·∫Ω h∆∞·ªõng d·∫´n chi ti·∫øt v·ªÅ gi·∫£i thu·∫≠t Non-Maximum Suppression (NMS) - m·ªôt k·ªπ thu·∫≠t quan tr·ªçng trong kh√¢u post-processing c·ªßa c√°c m√¥ h√¨nh YOLO. ƒê√¢y l√† ki·∫øn th·ª©c b·∫Øt bu·ªôc khi b·∫°n mu·ªën deploy YOLO model l√™n production.

## üìë M·ª•c L·ª•c

1. [NMS Gi·∫£i Quy·∫øt V·∫•n ƒê·ªÅ G√¨?](#1-nms-gi·∫£i-quy·∫øt-v·∫•n-ƒë·ªÅ-g√¨)
2. [Intersection over Union (IoU)](#2-intersection-over-union-iou)
3. [Thu·∫≠t To√°n NMS Ho·∫°t ƒê·ªông Nh∆∞ Th·∫ø N√†o?](#3-thu·∫≠t-to√°n-nms-ho·∫°t-ƒë·ªông-nh∆∞-th·∫ø-n√†o)
4. [Implement NMS T·ª´ ƒê·∫ßu](#4-implement-nms-t·ª´-ƒë·∫ßu)
5. [Tips & Best Practices](#5-tips--best-practices)
6. [FAQs - C√¢u H·ªèi Th∆∞·ªùng G·∫∑p](#6-faq---c√¢u-h·ªèi-th∆∞·ªùng-g·∫∑p)

---

## 1. NMS Gi·∫£i Quy·∫øt V·∫•n ƒê·ªÅ G√¨?

### 1.1. V·∫•n ƒê·ªÅ Khi Deploy YOLO Models

N·∫øu b·∫°n ƒë√£ t·ª´ng s·ª≠ d·ª•ng th∆∞ vi·ªán **Ultralytics** ƒë·ªÉ ch·∫°y YOLOv8, YOLOv5, output m√† b·∫°n nh·∫≠n ƒë∆∞·ª£c th∆∞·ªùng ·ªü ƒë·ªãnh d·∫°ng ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω s·∫µn:

```
x_center, y_center, width, height, class, confidence_score
```

Tuy nhi√™n, ƒë√¢y l√† output **ƒë√£ ƒë∆∞·ª£c h·∫≠u x·ª≠ l√Ω**. Khi b·∫°n convert model sang c√°c format kh√°c nh∆∞:
- **ONNX** (cho cross-platform deployment)
- **TFLite** (cho mobile devices)
- **TensorRT** (cho NVIDIA GPUs)
- **OpenVINO** (cho Intel hardware)
- **CoreML** (cho iOS devices)

Th√¨ output th√¥ c·ªßa model s·∫Ω c√≥ d·∫°ng:

```
Shape: (1, 84, 8400) ho·∫∑c (1, 8400, 84)
```

Trong ƒë√≥:
- **8400**: S·ªë l∆∞·ª£ng detection boxes (anchor points t·ª´ c√°c feature maps)
  - 80√ó80 = 6,400 (stride 8)
  - 40√ó40 = 1,600 (stride 16) 
  - 20√ó20 = 400 (stride 32)
  - **T·ªïng: 8,400 predictions**
- **84**: `4 t·ªça ƒë·ªô + 80 classes` (v·ªõi COCO dataset)

### 1.2. T·∫°i Sao C·∫ßn Ph·∫£i Hi·ªÉu NMS?

Khi deploy tr√™n c√°c n·ªÅn t·∫£ng kh√°c nhau:

| Platform | Ng√¥n Ng·ªØ | C·∫ßn Implement NMS |
|----------|----------|-------------------|
| Web | JavaScript | ‚úÖ C√≥ |
| Mobile (Android) | Java/Kotlin | ‚úÖ C√≥ |
| Mobile (iOS) | Swift | ‚úÖ C√≥ |
| Embedded | C++ | ‚úÖ C√≥ |
| Edge Devices | C/C++ | ‚úÖ C√≥ |

**Kh√¢u h·∫≠u x·ª≠ l√Ω (post-processing) c·ªßa t√°c gi·∫£ vi·∫øt b·∫±ng Python s·∫Ω KH√îNG ƒë∆∞·ª£c ƒë√≠nh k√®m** sau khi convert. ƒêi·ªÅu n√†y bu·ªôc b·∫°n ph·∫£i t·ª± implement l·∫°i NMS.

### 1.3. NMS L√†m G√¨?

![Illustration of NMS Problem](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/07/Screenshot-from-2020-07-27-20-37-58.png)
*H√¨nh 1: Tr∆∞·ªõc NMS (tr√°i) vs Sau NMS (ph·∫£i)*

**NMS lo·∫°i b·ªè c√°c bounding boxes tr√πng l·∫∑p v√† ch·ªâ gi·ªØ l·∫°i nh·ªØng boxes t·ªët nh·∫•t** cho m·ªói object.

V√≠ d·ª•: Khi detect m·ªôt con ch√≥, model c√≥ th·ªÉ t·∫°o ra 50-100 boxes cho c√πng 1 object. NMS s·∫Ω ch·ªçn box t·ªët nh·∫•t v√† lo·∫°i b·ªè c√°c boxes c√≤n l·∫°i.

---

## 2. Intersection over Union (IoU)

IoU l√† n·ªÅn t·∫£ng c·ªßa thu·∫≠t to√°n NMS. ƒê√¢y l√† metric ƒëo ƒë·ªô tr√πng l·∫∑p gi·ªØa 2 bounding boxes.

### 2.1. C√¥ng Th·ª©c IoU

![IoU Formula](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2016/09/iou_equation.png?lossy=2&strip=1&webp=1)
*H√¨nh 2: C√¥ng th·ª©c t√≠nh IoU*

```
IoU = Area of Intersection / Area of Union
```

### 2.2. V√≠ D·ª• T√≠nh IoU

![IoU Calculation Example](https://viso.ai/wp-content/uploads/2024/01/Illustrative-Example-for-IoU-Calculation.jpg)
*H√¨nh 3: V√≠ d·ª• minh h·ªça t√≠nh IoU*

Cho 2 boxes:
- **Box 1**: G√≥c tr√°i tr√™n (50, 100), g√≥c ph·∫£i d∆∞·ªõi (200, 300)
- **Box 2**: G√≥c tr√°i tr√™n (80, 120), g√≥c ph·∫£i d∆∞·ªõi (220, 310)

**B∆∞·ªõc 1: T√≠nh t·ªça ƒë·ªô ph·∫ßn giao (Intersection)**
```python
x_left = max(50, 80) = 80
y_top = max(100, 120) = 120
x_right = min(200, 220) = 200
y_bottom = min(300, 310) = 300
```

**B∆∞·ªõc 2: T√≠nh di·ªán t√≠ch giao**
```python
intersection_width = 200 - 80 = 120
intersection_height = 300 - 120 = 180
intersection_area = 120 √ó 180 = 21,600
```

**B∆∞·ªõc 3: T√≠nh di·ªán t√≠ch h·ª£p**
```python
box1_area = (200 - 50) √ó (300 - 100) = 150 √ó 200 = 30,000
box2_area = (220 - 80) √ó (310 - 120) = 140 √ó 190 = 26,600
union_area = box1_area + box2_area - intersection_area
union_area = 30,000 + 26,600 - 21,600 = 35,000
```

**B∆∞·ªõc 4: T√≠nh IoU**
```python
IoU = 21,600 / 35,000 = 0.617 (‚âà 61.7%)
```

### 2.3. Code Implementation IoU

```python
def calculate_iou(box1, box2):
    """
    T√≠nh IoU gi·ªØa 2 bounding boxes
    
    Args:
        box1: [x1, y1, x2, y2] - t·ªça ƒë·ªô g√≥c tr√°i tr√™n v√† ph·∫£i d∆∞·ªõi
        box2: [x1, y1, x2, y2]
    
    Returns:
        iou: float - gi√° tr·ªã IoU t·ª´ 0 ƒë·∫øn 1
    """
    # T√¨m t·ªça ƒë·ªô v√πng giao
    x_left = max(box1[0], box2[0])
    y_top = max(box1[1], box2[1])
    x_right = min(box1[2], box2[2])
    y_bottom = min(box1[3], box2[3])
    
    # Ki·ªÉm tra c√≥ giao kh√¥ng
    if x_right < x_left or y_bottom < y_top:
        return 0.0
    
    # T√≠nh di·ªán t√≠ch giao
    intersection_area = (x_right - x_left) * (y_bottom - y_top)
    
    # T√≠nh di·ªán t√≠ch m·ªói box
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    
    # T√≠nh di·ªán t√≠ch h·ª£p
    union_area = box1_area + box2_area - intersection_area
    
    # T√≠nh IoU
    iou = intersection_area / union_area if union_area > 0 else 0
    
    return iou

# Test function
box1 = [50, 100, 200, 300]
box2 = [80, 120, 220, 310]
print(f"IoU = {calculate_iou(box1, box2):.4f}")  # Output: IoU = 0.6171
```

### 2.4. √ù Nghƒ©a C·ªßa IoU Threshold

| IoU Value | √ù Nghƒ©a | Quy·∫øt ƒê·ªãnh NMS |
|-----------|---------|----------------|
| 0.0 - 0.3 | Kh√¥ng tr√πng ho·∫∑c tr√πng √≠t | ‚úÖ Gi·ªØ c·∫£ 2 boxes |
| 0.3 - 0.5 | Tr√πng v·ª´a ph·∫£i | ‚ö†Ô∏è T√πy threshold |
| 0.5 - 0.7 | Tr√πng nhi·ªÅu | ‚ùå Lo·∫°i box c√≥ conf th·∫•p |
| 0.7 - 1.0 | Tr√πng r·∫•t nhi·ªÅu | ‚ùå Ch·∫Øc ch·∫Øn lo·∫°i |

**Recommended IoU thresholds:**
- **0.45 - 0.5**: Cho objects nh·ªè, chen ch√∫c (crowded scenes)
- **0.5 - 0.6**: C√¢n b·∫±ng t·ªët cho h·∫ßu h·∫øt tr∆∞·ªùng h·ª£p
- **0.6 - 0.7**: Cho objects l·ªõn, r·ªùi r·∫°c

---

## 3. Thu·∫≠t To√°n NMS Ho·∫°t ƒê·ªông Nh∆∞ Th·∫ø N√†o?

### 3.1. Overview - Quy Tr√¨nh 5 B∆∞·ªõc

```
Input: 8400 predictions (m·ªói prediction c√≥ 84 gi√° tr·ªã)
       ‚Üì
[Step 1] Filter by Confidence Threshold
       ‚Üì (gi·ªØ l·∫°i ~100-500 boxes)
[Step 2] Extract Class & Confidence
       ‚Üì
[Step 3] Group by Class
       ‚Üì
[Step 4] Apply NMS per Class
       ‚Üì
[Step 5] Combine Results
       ‚Üì
Output: Final detections (x, y, w, h, class, conf)
```

### 3.2. Chi Ti·∫øt T·ª´ng B∆∞·ªõc

#### **Step 1: L·ªçc Theo Confidence Threshold**

**Input format**: M·ªói detection c√≥ d·∫°ng vector 84 gi√° tr·ªã:
```
[x_center, y_center, width, height, score_class0, score_class1, ..., score_class79]
 ‚Üë________bbox coords_______‚Üë  ‚Üë____________80 class scores_________________‚Üë
```

**V√≠ d·ª• minh h·ªça:**

Gi·∫£ s·ª≠ detection vector:
```python
detection = [0.5, 0.3, 0.2, 0.15,  # bbox: x, y, w, h
             0.02, 0.91, 0.05, ..., 0.01]  # 80 class scores
```

V·ªõi `conf_threshold = 0.5`:
```python
max_score = max(detection[4:]) = 0.91  # score c·ªßa class 1 (person)
if max_score > 0.5:
    keep_this_detection = True  # ‚úÖ GI·ªÆ L·∫†I
else:
    discard_detection = True    # ‚ùå LO·∫†I B·ªé
```

**T·∫°i sao c√°c scores kh√¥ng t·ªïng = 1?**

YOLO s·ª≠ d·ª•ng **Sigmoid activation** cho m·ªói class (kh√¥ng ph·∫£i Softmax):
- Cho ph√©p multi-label detection (1 object c√≥ th·ªÉ thu·ªôc nhi·ªÅu class)
- M·ªói class score ƒë·ªôc l·∫≠p: `sigmoid(x) ‚àà [0, 1]`

V√≠ d·ª•: M·ªôt object c√≥ th·ªÉ v·ª´a l√† "person" (0.9) v·ª´a l√† "tennis racket" (0.8)

#### **Step 2: Extract Class ID & Max Confidence**

```python
# Tr∆∞·ªõc Step 2
detection = [0.5, 0.3, 0.2, 0.15, 0.02, 0.91, 0.05, ..., 0.01]

# Sau Step 2
class_id = argmax([0.02, 0.91, 0.05, ..., 0.01]) = 1  # class "person"
max_conf = max([0.02, 0.91, 0.05, ..., 0.01]) = 0.91

# Output format m·ªõi
processed_detection = [0.5, 0.3, 0.2, 0.15, 1, 0.91]
#                      ‚Üë___bbox coords___‚Üë  ‚Üëcls‚Üëconf
```

**Note**: Trong th·ª±c t·∫ø, Step 1 v√† Step 2 ƒë∆∞·ª£c th·ª±c hi·ªán c√πng l√∫c ƒë·ªÉ t·ªëi ∆∞u performance.

#### **Step 3: Group Detections By Class**

M·ª•c ƒë√≠ch: T√°ch c√°c detections theo t·ª´ng class ƒë·ªÉ x·ª≠ l√Ω ri√™ng bi·ªát.

```python
# Input: T·∫•t c·∫£ detections sau Step 2
all_detections = [
    [100, 200, 50, 80, 0, 0.9],   # person
    [150, 220, 48, 75, 0, 0.85],  # person (tr√πng v·ªõi detection tr√™n)
    [500, 300, 60, 90, 1, 0.92],  # bicycle
    [120, 210, 52, 78, 0, 0.88],  # person (c≈©ng tr√πng)
    [510, 305, 58, 88, 1, 0.87],  # bicycle (tr√πng v·ªõi bicycle tr√™n)
]

# Output: Grouped by class
class_0_detections = [  # person
    [100, 200, 50, 80, 0, 0.9],
    [150, 220, 48, 75, 0, 0.85],
    [120, 210, 52, 78, 0, 0.88],
]

class_1_detections = [  # bicycle
    [500, 300, 60, 90, 1, 0.92],
    [510, 305, 58, 88, 1, 0.87],
]
```

**T·∫°i sao ph·∫£i group by class?**
- Tr√°nh lo·∫°i b·ªè nh·∫ßm detections c·ªßa c√°c objects kh√°c nhau
- V√≠ d·ª•: Person box v√† Car box c√≥ th·ªÉ overlap nh∆∞ng l√† 2 objects kh√°c nhau

#### **Step 4: Apply NMS Per Class** (B∆∞·ªõc quan tr·ªçng nh·∫•t!)

**Thu·∫≠t to√°n NMS cho 1 class:**

```python
def nms_single_class(detections, iou_threshold=0.5):
    """
    Input: 
      detections = [[x1,y1,x2,y2,class,conf], ...]  # N detections
      iou_threshold = 0.5
    
    Output:
      kept_detections = [[x1,y1,x2,y2,class,conf], ...]  # M detections (M < N)
    """
    # B∆∞·ªõc 1: S·∫Øp x·∫øp theo confidence score gi·∫£m d·∫ßn
    sorted_detections = sort_by_confidence_desc(detections)
    
    kept_boxes = []
    
    # B∆∞·ªõc 2: Loop qua t·ª´ng detection
    while len(sorted_detections) > 0:
        # L·∫•y box c√≥ confidence cao nh·∫•t
        best_box = sorted_detections[0]
        kept_boxes.append(best_box)
        
        # B∆∞·ªõc 3: T√≠nh IoU c·ªßa best_box v·ªõi t·∫•t c·∫£ boxes c√≤n l·∫°i
        remaining_boxes = []
        for box in sorted_detections[1:]:
            iou = calculate_iou(best_box, box)
            
            # N·∫øu IoU < threshold ‚Üí gi·ªØ l·∫°i (kh√¥ng tr√πng)
            if iou < iou_threshold:
                remaining_boxes.append(box)
            # N·∫øu IoU >= threshold ‚Üí lo·∫°i b·ªè (tr√πng l·∫∑p)
        
        sorted_detections = remaining_boxes
    
    return kept_boxes
```

**Minh h·ªça b·∫±ng v√≠ d·ª• c·ª• th·ªÉ:**

```python
# Gi·∫£ s·ª≠ c√≥ 4 person detections (ƒë√£ sort theo conf)
person_detections = [
    [100, 200, 150, 280, 0, 0.92],  # Box A - BEST
    [105, 205, 155, 285, 0, 0.88],  # Box B - overlap v·ªõi A
    [110, 202, 152, 282, 0, 0.85],  # Box C - overlap v·ªõi A
    [400, 300, 450, 400, 0, 0.80],  # Box D - person kh√°c
]

# Iteration 1:
# - Ch·ªçn Box A (conf=0.92) ‚Üí KEEP
# - T√≠nh IoU(A, B) = 0.85 > 0.5 ‚Üí REMOVE B
# - T√≠nh IoU(A, C) = 0.78 > 0.5 ‚Üí REMOVE C
# - T√≠nh IoU(A, D) = 0.02 < 0.5 ‚Üí KEEP D
# Remaining: [Box D]

# Iteration 2:
# - Ch·ªçn Box D (conf=0.80) ‚Üí KEEP
# Remaining: []

# Final result:
kept_boxes = [
    [100, 200, 150, 280, 0, 0.92],  # Box A
    [400, 300, 450, 400, 0, 0.80],  # Box D
]
```

**Flowchart c·ªßa NMS:**

```
START
  ‚Üì
Sort boxes by confidence (desc)
  ‚Üì
Pick highest confidence box ‚Üí ADD to output
  ‚Üì
Calculate IoU with all remaining boxes
  ‚Üì
Remove boxes with IoU > threshold
  ‚Üì
Any boxes left? 
  ‚îú‚îÄ YES ‚Üí Go back to "Pick highest"
  ‚îî‚îÄ NO ‚Üí END
```

#### **Step 5: Combine All Classes**

```python
# K·∫øt h·ª£p k·∫øt qu·∫£ t·ª´ t·∫•t c·∫£ c√°c classes
final_detections = []

for class_id in range(num_classes):
    class_detections = group_by_class[class_id]
    kept_detections = nms_single_class(class_detections, iou_threshold=0.5)
    final_detections.extend(kept_detections)

# final_detections ch√≠nh l√† output cu·ªëi c√πng!
```

### 3.3. Class-Aware vs Class-Agnostic NMS

| | Class-Aware NMS | Class-Agnostic NMS |
|---|---|---|
| **C√°ch ho·∫°t ƒë·ªông** | Ch·∫°y NMS ri√™ng cho t·ª´ng class | Ch·∫°y NMS tr√™n T·∫§T C·∫¢ boxes c√πng l√∫c |
| **∆Øu ƒëi·ªÉm** | Kh√¥ng lo·∫°i nh·∫ßm objects kh√°c class | Nhanh h∆°n, ƒë∆°n gi·∫£n h∆°n |
| **Nh∆∞·ª£c ƒëi·ªÉm** | Ch·∫≠m h∆°n khi c√≥ nhi·ªÅu classes | C√≥ th·ªÉ lo·∫°i nh·∫ßm overlapping objects |
| **Use case** | YOLO standard, crowded scenes | Real-time apps, single/few classes |

**V√≠ d·ª• so s√°nh:**

```python
# Scenario: Person ƒë·ª©ng c·∫°nh bicycle, 2 boxes overlap 60%
person_box = [100, 200, 150, 300, 0, 0.9]   # class 0: person
bicycle_box = [120, 210, 180, 310, 1, 0.85] # class 1: bicycle

# Class-Aware NMS:
# ‚Üí X·ª≠ l√Ω ri√™ng person v√† bicycle
# ‚Üí GI·ªÆ C·∫¢ 2 BOXES ‚úÖ

# Class-Agnostic NMS:
# ‚Üí X·ª≠ l√Ω chung t·∫•t c·∫£
# ‚Üí IoU = 0.6 > threshold
# ‚Üí Ch·ªâ gi·ªØ person_box (conf cao h∆°n) ‚ùå SAI!
```

**K·∫øt lu·∫≠n**: Class-Aware NMS t·ªët h∆°n cho general object detection.

---

## 4. Implement NMS T·ª´ ƒê·∫ßu

### 4.1. Setup Environment

```python
# Import th∆∞ vi·ªán c·∫ßn thi·∫øt
import numpy as np
import matplotlib.pyplot as plt
import cv2
from ultralytics import YOLO
import onnxruntime as ort

# Load ONNX model
model = ort.InferenceSession(
    "./yolov8n.onnx", 
    providers=["CPUExecutionProvider"]
)

# Ki·ªÉm tra input/output shape
inputs = model.get_inputs()
outputs = model.get_outputs()

print("=" * 50)
print("MODEL INFORMATION")
print("=" * 50)
print(f"Input Name:  {inputs[0].name}")
print(f"Input Type:  {inputs[0].type}")
print(f"Input Shape: {inputs[0].shape}")
print("-" * 50)
print(f"Output Name:  {outputs[0].name}")
print(f"Output Type:  {outputs[0].type}")
print(f"Output Shape: {outputs[0].shape}")
print("=" * 50)
```

**Expected Output:**
```
==================================================
MODEL INFORMATION
==================================================
Input Name:  images
Input Type:  tensor(float)
Input Shape: [1, 3, 640, 640]
--------------------------------------------------
Output Name:  output0
Output Type:  tensor(float)
Output Shape: [1, 84, 8400]
==================================================
```

### 4.2. Image Preprocessing

```python
def preprocess_image(image_path, target_size=640):
    """
    Ti·ªÅn x·ª≠ l√Ω ·∫£nh cho YOLO model
    
    Args:
        image_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn ·∫£nh
        target_size: K√≠ch th∆∞·ªõc input c·ªßa model (m·∫∑c ƒë·ªãnh 640)
    
    Returns:
        original_image: ·∫¢nh g·ªëc (BGR format)
        preprocessed_image: ·∫¢nh ƒë√£ x·ª≠ l√Ω (shape: 1, 3, 640, 640)
    """
    # ƒê·ªçc ·∫£nh
    image = cv2.imread(image_path)
    original_image = image.copy()
    
    print(f"üì∑ Original image shape: {image.shape}")
    
    # Convert BGR ‚Üí RGB
    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # Resize v·ªÅ 640x640
    resized_image = cv2.resize(rgb_image, (target_size, target_size))
    
    # Normalize [0, 255] ‚Üí [0, 1]
    normalized_image = resized_image.astype("float32") / 255.0
    
    # Transpose (H, W, C) ‚Üí (C, H, W)
    transposed_image = normalized_image.transpose(2, 0, 1)
    
    # Add batch dimension (C, H, W) ‚Üí (1, C, H, W)
    preprocessed_image = transposed_image[np.newaxis, ...]
    
    print(f"üîß Preprocessed shape: {preprocessed_image.shape}")
    
    return original_image, preprocessed_image

# Test preprocessing
original_img, preprocessed_img = preprocess_image("./demo.jpg")
```

### 4.3. Run Inference

```python
# Ch·∫°y model inference
predictions = model.run(
    ["output0"],  # output names
    {"images": preprocessed_img}  # input dict
)[0]

print(f"üéØ Raw prediction shape: {predictions.shape}")
# Output: (1, 84, 8400)

# Transpose ƒë·ªÉ d·ªÖ x·ª≠ l√Ω
predictions = predictions.transpose(0, 2, 1)[0]
print(f"üîÑ Transposed shape: {predictions.shape}")
# Output: (8400, 84)
```

### 4.4. Implement NMS - Complete Code

#### **Function 1: Parse & Filter Predictions (Step 1 & 2)**

```python
def parse_predictions(
    predictions, 
    img_width, 
    img_height, 
    conf_threshold=0.5,
    model_input_size=640
):
    """
    Parse YOLO predictions v√† filter theo confidence threshold
    
    Args:
        predictions: Array shape (8400, 84) t·ª´ YOLO model
        img_width: Chi·ªÅu r·ªông ·∫£nh g·ªëc
        img_height: Chi·ªÅu cao ·∫£nh g·ªëc
        conf_threshold: Ng∆∞·ª°ng confidence t·ªëi thi·ªÉu (default: 0.5)
        model_input_size: K√≠ch th∆∞·ªõc input model (default: 640)
    
    Returns:
        filtered_boxes: Array shape (N, 6) v·ªõi format:
                       [x1, y1, x2, y2, class_id, confidence]
    """
    # Step 1: T√≠nh max confidence score cho m·ªói detection
    # predictions[:, 4:] l·∫•y 80 class scores
    max_conf_scores = np.max(predictions[:, 4:], axis=1)
    
    # Step 2: L·ªçc detections c√≥ conf > threshold
    valid_indices = np.where(max_conf_scores > conf_threshold)[0]
    
    if len(valid_indices) == 0:
        print("‚ö†Ô∏è  No detections above confidence threshold!")
        return np.array([])
    
    # L·∫•y c√°c detections h·ª£p l·ªá
    valid_detections = predictions[valid_indices]
    
    print(f"‚úÖ Filtered: {len(valid_detections)}/{len(predictions)} detections")
    
    # Step 3: Extract class_id v√† confidence
    class_ids = valid_detections[:, 4:].argmax(axis=1)
    confidences = valid_detections[:, 4:].max(axis=1)
    
    # Step 4: Convert t·ª´ center format ‚Üí corner format
    # YOLO output: [x_center, y_center, width, height] (normalized 0-1)
    # C·∫ßn convert sang: [x1, y1, x2, y2] (pixel coordinates)
    
    x_centers = valid_detections[:, 0]
    y_centers = valid_detections[:, 1]
    widths = valid_detections[:, 2]
    heights = valid_detections[:, 3]
    
    # Rescale v·ªÅ t·ªça ƒë·ªô ·∫£nh g·ªëc
    scale_x = img_width / model_input_size
    scale_y = img_height / model_input_size
    
    x1 = ((x_centers - widths / 2) / model_input_size) * img_width
    y1 = ((y_centers - heights / 2) / model_input_size) * img_height
    x2 = ((x_centers + widths / 2) / model_input_size) * img_width
    y2 = ((y_centers + heights / 2) / model_input_size) * img_height
    
    # Clip coordinates v·ªÅ gi·ªõi h·∫°n ·∫£nh
    x1 = np.clip(x1, 0, img_width)
    y1 = np.clip(y1, 0, img_height)
    x2 = np.clip(x2, 0, img_width)
    y2 = np.clip(y2, 0, img_height)
    
    # Combine t·∫•t c·∫£ th√¥ng tin
    filtered_boxes = np.hstack([
        x1[:, np.newaxis],
        y1[:, np.newaxis],
        x2[:, np.newaxis],
        y2[:, np.newaxis],
        class_ids[:, np.newaxis],
        confidences[:, np.newaxis]
    ])
    
    return filtered_boxes
```

#### **Function 2: NMS for Single Class (Step 4)**

```python
def nms_single_class(boxes, scores, iou_threshold=0.5):
    """
    Apply Non-Maximum Suppression cho 1 class
    
    Args:
        boxes: Array shape (N, 4) v·ªõi format [x1, y1, x2, y2]
        scores: Array shape (N,) - confidence scores
        iou_threshold: Ng∆∞·ª°ng IoU ƒë·ªÉ lo·∫°i b·ªè boxes tr√πng l·∫∑p
    
    Returns:
        keep_indices: Array indices c·ªßa c√°c boxes ƒë∆∞·ª£c gi·ªØ l·∫°i
    """
    if len(boxes) == 0:
        return np.array([], dtype=np.int32)
    
    # Extract coordinates
    x1 = boxes[:, 0]
    y1 = boxes[:, 1]
    x2 = boxes[:, 2]
    y2 = boxes[:, 3]
    
    # T√≠nh di·ªán t√≠ch m·ªói box
    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    
    # Sort indices theo confidence score gi·∫£m d·∫ßn
    order = scores.argsort()[::-1]
    
    keep = []
    
    while order.size > 0:
        # L·∫•y index c·ªßa box c√≥ confidence cao nh·∫•t
        i = order[0]
        keep.append(i)
        
        # N·∫øu ch·ªâ c√≤n 1 box ‚Üí k·∫øt th√∫c
        if order.size == 1:
            break
        
        # T√≠nh IoU c·ªßa box hi·ªán t·∫°i v·ªõi t·∫•t c·∫£ boxes c√≤n l·∫°i
        # T√¨m t·ªça ƒë·ªô v√πng giao
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])
        
        # T√≠nh width v√† height c·ªßa v√πng giao
        w = np.maximum(0.0, xx2 - xx1 + 1)
        h = np.maximum(0.0, yy2 - yy1 + 1)
        
        # T√≠nh di·ªán t√≠ch giao
        intersection = w * h
        
        # T√≠nh IoU
        iou = intersection / (areas[i] + areas[order[1:]] - intersection)
        
        # Gi·ªØ l·∫°i c√°c boxes c√≥ IoU <= threshold (kh√¥ng tr√πng)
        inds = np.where(iou <= iou_threshold)[0]
        
        # Update order (c·ªông 1 v√¨ ƒë√£ b·ªè qua ph·∫ßn t·ª≠ ƒë·∫ßu)
        order = order[inds + 1]
    
    return np.array(keep, dtype=np.int32)
```

#### **Function 3: Apply NMS All Classes (Step 3, 4, 5)**

```python
def apply_nms(filtered_boxes, iou_threshold=0.5):
    """
    Apply Class-Aware NMS tr√™n t·∫•t c·∫£ detections
    
    Args:
        filtered_boxes: Array shape (N, 6) 
                       [x1, y1, x2, y2, class_id, confidence]
        iou_threshold: Ng∆∞·ª°ng IoU (default: 0.5)
    
    Returns:
        final_boxes: Array c√°c boxes sau NMS
    """
    if len(filtered_boxes) == 0:
        return np.array([])
    
    # Step 3: L·∫•y danh s√°ch unique classes
    unique_classes = np.unique(filtered_boxes[:, 4])
    
    print(f"\nüé® Detected classes: {unique_classes.astype(int)}")
    
    final_indices = []
    
    # Step 4: Loop qua t·ª´ng class v√† apply NMS
    for class_id in unique_classes:
        # L·∫•y indices c·ªßa class n√†y
        class_indices = np.where(filtered_boxes[:, 4] == class_id)[0]
        class_boxes = filtered_boxes[class_indices, :4]  # x1,y1,x2,y2
        class_scores = filtered_boxes[class_indices, 5]  # confidence
        
        print(f"  Class {int(class_id)}: {len(class_boxes)} boxes before NMS")
        
        # Apply NMS cho class n√†y
        keep_indices = nms_single_class(
            class_boxes, 
            class_scores, 
            iou_threshold
        )
        
        # Chuy·ªÉn v·ªÅ indices global
        global_keep_indices = class_indices[keep_indices]
        final_indices.extend(global_keep_indices)
        
        print(f"  Class {int(class_id)}: {len(keep_indices)} boxes after NMS ‚úì")
    
    # Step 5: Combine results
    final_boxes = filtered_boxes[final_indices]
    
    print(f"\nüì¶ Final detections: {len(final_boxes)}")
    
    return final_boxes
```

### 4.5. Complete Pipeline

```python
# COCO class names (80 classes)
COCO_CLASSES = [
    "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", 
    "truck", "boat", "traffic light", "fire hydrant", "stop sign", 
    "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow",
    "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella", 
    "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", 
    "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", 
    "surfboard", "tennis racket", "bottle", "wine glass", "cup", "fork", 
    "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange", 
    "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair", 
    "couch", "potted plant", "bed", "dining table", "toilet", "tv", 
    "laptop", "mouse", "remote", "keyboard", "cell phone", "microwave", 
    "oven", "toaster", "sink", "refrigerator", "book", "clock", "vase", 
    "scissors", "teddy bear", "hair drier", "toothbrush"
]

def yolo_postprocess_complete(
    image_path,
    model,
    conf_threshold=0.5,
    iou_threshold=0.5
):
    """
    Complete YOLO post-processing pipeline
    
    Args:
        image_path: ƒê∆∞·ªùng d·∫´n ·∫£nh
        model: ONNX model session
        conf_threshold: Confidence threshold
        iou_threshold: IoU threshold for NMS
    
    Returns:
        final_detections: Array c√°c detections cu·ªëi c√πng
        original_image: ·∫¢nh g·ªëc ƒë·ªÉ visualize
    """
    print("\n" + "="*60)
    print("YOLO POST-PROCESSING PIPELINE")
    print("="*60)
    
    # 1. Preprocess
    print("\n[1/5] Preprocessing image...")
    original_image, preprocessed_image = preprocess_image(image_path)
    img_height, img_width = original_image.shape[:2]
    
    # 2. Inference
    print("\n[2/5] Running model inference...")
    predictions = model.run(
        ["output0"], 
        {"images": preprocessed_image}
    )[0]
    predictions = predictions.transpose(0, 2, 1)[0]  # (8400, 84)
    
    # 3. Parse & Filter
    print(f"\n[3/5] Parsing predictions (conf > {conf_threshold})...")
    filtered_boxes = parse_predictions(
        predictions,
        img_width,
        img_height,
        conf_threshold=conf_threshold
    )
    
    if len(filtered_boxes) == 0:
        print("\n‚ùå No detections found!")
        return np.array([]), original_image
    
    # 4. Apply NMS
    print(f"\n[4/5] Applying NMS (IoU threshold: {iou_threshold})...")
    final_detections = apply_nms(filtered_boxes, iou_threshold=iou_threshold)
    
    # 5. Print results
    print("\n[5/5] Final Results:")
    print("="*60)
    for det in final_detections:
        x1, y1, x2, y2, class_id, conf = det
        class_name = COCO_CLASSES[int(class_id)]
        print(f"  üìç {class_name:15s} | Conf: {conf:.3f} | "
              f"Box: [{int(x1)}, {int(y1)}, {int(x2)}, {int(y2)}]")
    print("="*60)
    
    return final_detections, original_image

# Run complete pipeline
final_dets, orig_img = yolo_postprocess_complete(
    image_path="./demo.jpg",
    model=model,
    conf_threshold=0.5,
    iou_threshold=0.5
)
```

### 4.6. Visualization

```python
def visualize_detections(image, detections, class_names=COCO_CLASSES):
    """
    V·∫Ω bounding boxes v√† labels l√™n ·∫£nh
    
    Args:
        image: ·∫¢nh g·ªëc (BGR format)
        detections: Array shape (N, 6)
        class_names: List t√™n c√°c classes
    """
    # Copy ·∫£nh ƒë·ªÉ kh√¥ng modify original
    vis_image = image.copy()
    
    # Generate colors cho m·ªói class
    np.random.seed(42)
    colors = np.random.randint(0, 255, size=(len(class_names), 3))
    
    for det in detections:
        x1, y1, x2, y2, class_id, conf = det
        class_id = int(class_id)
        
        # Get color v√† class name
        color = colors[class_id].tolist()
        class_name = class_names[class_id]
        
        # V·∫Ω bounding box
        cv2.rectangle(
            vis_image,
            (int(x1), int(y1)),
            (int(x2), int(y2)),
            color,
            thickness=2
        )
        
        # T·∫°o label text
        label = f"{class_name} {conf:.2f}"
        
        # T√≠nh k√≠ch th∆∞·ªõc text
        (text_width, text_height), _ = cv2.getTextSize(
            label,
            cv2.FONT_HERSHEY_SIMPLEX,
            0.6,
            1
        )
        
        # V·∫Ω background cho text
        cv2.rectangle(
            vis_image,
            (int(x1), int(y1) - text_height - 10),
            (int(x1) + text_width, int(y1)),
            color,
            -1
        )
        
        # V·∫Ω text
        cv2.putText(
            vis_image,
            label,
            (int(x1), int(y1) - 5),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.6,
            (255, 255, 255),
            thickness=2
        )
    
    return vis_image

# Visualize k·∫øt qu·∫£
result_image = visualize_detections(orig_img, final_dets)

# Display
plt.figure(figsize=(12, 8))
plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))
plt.axis('off')
plt.title('YOLO Detection Results After NMS', fontsize=16)
plt.tight_layout()
plt.savefig('detection_result.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úÖ Saved result to 'detection_result.png'")
```

### 4.7. Performance Benchmarking

```python
import time

def benchmark_nms(image_path, model, num_runs=10):
    """
    ƒêo performance c·ªßa NMS pipeline
    """
    print(f"\n‚è±Ô∏è  Running benchmark ({num_runs} iterations)...")
    
    times = {
        'preprocess': [],
        'inference': [],
        'postprocess': [],
        'total': []
    }
    
    for i in range(num_runs):
        t_total_start = time.time()
        
        # Preprocess
        t1 = time.time()
        orig_img, prep_img = preprocess_image(image_path)
        t2 = time.time()
        times['preprocess'].append(t2 - t1)
        
        # Inference
        t1 = time.time()
        pred = model.run(["output0"], {"images": prep_img})[0]
        pred = pred.transpose(0, 2, 1)[0]
        t2 = time.time()
        times['inference'].append(t2 - t1)
        
        # Postprocess
        t1 = time.time()
        filtered = parse_predictions(pred, orig_img.shape[1], orig_img.shape[0])
        final = apply_nms(filtered)
        t2 = time.time()
        times['postprocess'].append(t2 - t1)
        
        times['total'].append(time.time() - t_total_start)
    
    # Print results
    print("\n" + "="*60)
    print("PERFORMANCE BENCHMARK RESULTS")
    print("="*60)
    for stage, stage_times in times.items():
        avg = np.mean(stage_times) * 1000
        std = np.std(stage_times) * 1000
        print(f"{stage:15s}: {avg:6.2f} ¬± {std:5.2f} ms")
    print("="*60)
    
    total_avg = np.mean(times['total']) * 1000
    fps = 1000 / total_avg
    print(f"Average FPS: {fps:.1f}")
    print("="*60)

# Run benchmark
benchmark_nms("./demo.jpg", model, num_runs=10)
```

---

## 5. Tips & Best Practices

### 5.1. Ch·ªçn Confidence Threshold

| Use Case | Recommended `conf_threshold` | L√Ω do |
|----------|------------------------------|-------|
| General purpose | 0.5 - 0.6 | C√¢n b·∫±ng precision/recall |
| High precision needed | 0.7 - 0.8 | Gi·∫£m false positives |
| High recall needed | 0.3 - 0.4 | Detect nhi·ªÅu objects, ch·∫•p nh·∫≠n FP |
| Real-time application | 0.6 - 0.7 | Gi·∫£m s·ªë boxes c·∫ßn x·ª≠ l√Ω ‚Üí nhanh h∆°n |

**C√°ch tuning:**
```python
# Test v·ªõi nhi·ªÅu thresholds
thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]
for thresh in thresholds:
    dets, _ = yolo_postprocess_complete(
        "test.jpg", model, 
        conf_threshold=thresh
    )
    print(f"Threshold {thresh}: {len(dets)} detections")
```

### 5.2. Ch·ªçn IoU Threshold

| Scenario | Recommended `iou_threshold` | V√≠ d·ª• |
|----------|----------------------------|-------|
| Crowded scenes | 0.3 - 0.4 | Crowd counting, traffic |
| Normal scenes | 0.5 - 0.6 | General detection |
| Sparse objects | 0.6 - 0.7 | Large objects, few overlaps |

**Rule of thumb:**
- IoU c√†ng **th·∫•p** ‚Üí c√†ng **aggressive** (lo·∫°i nhi·ªÅu boxes)
- IoU c√†ng **cao** ‚Üí c√†ng **conservative** (gi·ªØ nhi·ªÅu boxes)

### 5.3. T·ªëi ∆Øu Performance

#### **Vectorized Operations**

```python
# ‚ùå BAD: Loop t·ª´ng box
def slow_nms(boxes, scores, iou_thresh):
    keep = []
    for i in range(len(boxes)):
        if i in removed:
            continue
        for j in range(i+1, len(boxes)):
            iou = calculate_iou(boxes[i], boxes[j])
            if iou > iou_thresh:
                removed.add(j)
    return keep

# ‚úÖ GOOD: Vectorized operations
def fast_nms(boxes, scores, iou_thresh):
    # T√≠nh t·∫•t c·∫£ IoUs c√πng l√∫c
    ious = calculate_iou_matrix(boxes)  # vectorized
    # ... rest of NMS logic
```

**Performance gain: ~10-50x faster!**

#### **Early Stopping**

```python
# N·∫øu s·ªë detections qu√° nhi·ªÅu ‚Üí tƒÉng conf_threshold
if len(filtered_boxes) > 1000:
    print("‚ö†Ô∏è Too many detections, increase conf_threshold!")
    filtered_boxes = filtered_boxes[filtered_boxes[:, 5] > 0.7]
```

### 5.4. Common Pitfalls

#### **Pitfall 1: Qu√™n Rescale Coordinates**

```python
# ‚ùå SAI: Gi·ªØ nguy√™n t·ªça ƒë·ªô normalized
x1 = (x_center - width/2)  # s·∫Ω ra gi√° tr·ªã 0-1

# ‚úÖ ƒê√öNG: Rescale v·ªÅ pixel coordinates
x1 = ((x_center - width/2) / 640) * img_width
```

#### **Pitfall 2: D√πng Class-Agnostic NMS Khi C√≥ Overlapping Objects**

```python
# ‚ùå SAI: NMS chung cho t·∫•t c·∫£ classes
all_boxes = combine_all_classes(detections)
keep = nms(all_boxes)  # C√≥ th·ªÉ lo·∫°i nh·∫ßm!

# ‚úÖ ƒê√öNG: NMS ri√™ng cho t·ª´ng class
for class_id in unique_classes:
    class_boxes = filter_by_class(detections, class_id)
    keep = nms(class_boxes)
```

#### **Pitfall 3: Kh√¥ng Clip Coordinates**

```python
# ‚ùå SAI: Coordinates c√≥ th·ªÉ √¢m ho·∫∑c v∆∞·ª£t qu√° ·∫£nh
x1, y1, x2, y2 = convert_coords(...)

# ‚úÖ ƒê√öNG: Clip v·ªÅ gi·ªõi h·∫°n ·∫£nh
x1 = np.clip(x1, 0, img_width)
y1 = np.clip(y1, 0, img_height)
x2 = np.clip(x2, 0, img_width)
y2 = np.clip(y2, 0, img_height)
```

### 5.5. Debugging Tips

```python
# Visualize intermediate results
def debug_nms(image, detections, stage_name):
    """V·∫Ω detections ·ªü m·ªói stage"""
    vis = visualize_detections(image, detections)
    cv2.imwrite(f"debug_{stage_name}.jpg", vis)
    print(f"üíæ Saved {stage_name}: {len(detections)} boxes")

# Usage
filtered = parse_predictions(...)
debug_nms(image, filtered, "after_filtering")

final = apply_nms(filtered)
debug_nms(image, final, "after_nms")
```

---

## 6. FAQ - C√¢u H·ªèi Th∆∞·ªùng G·∫∑p

### Q1: T·∫°i sao sau khi convert sang ONNX, s·ªë l∆∞·ª£ng detections l·∫°i nhi·ªÅu h∆°n khi d√πng Ultralytics?

**A:** V√¨ Ultralytics ƒë√£ apply NMS v·ªõi default settings (`conf=0.25, iou=0.45`). Khi b·∫°n convert, b·∫°n nh·∫≠n raw output ch∆∞a qua NMS.

```python
# Ultralytics internals (hidden from user)
results = model.predict("image.jpg")  
# ‚Üë ƒê√£ apply NMS inside

# ONNX (raw output)
raw_output = onnx_model.run(...)
# ‚Üë Ch∆∞a apply NMS, b·∫°n ph·∫£i t·ª± l√†m
```

### Q2: Class-Aware NMS c√≥ ch·∫≠m h∆°n Class-Agnostic NMS nhi·ªÅu kh√¥ng?

**A:** C√≥, nh∆∞ng kh√¥ng ƒë√°ng k·ªÉ trong most cases:

```python
# Benchmark results (10,000 detections, 80 classes)
Class-Agnostic NMS: ~15ms
Class-Aware NMS:    ~25ms
# Ch·ªâ ch·∫≠m h∆°n ~10ms, acceptable!
```

Trade-off n√†y **ƒë√°ng gi√°** v√¨ accuracy cao h∆°n nhi·ªÅu.

### Q3: L√†m sao ƒë·ªÉ detect objects r·∫•t nh·ªè (small objects)?

**Tips:**
1. **Gi·∫£m `conf_threshold`**: 0.3 - 0.4 thay v√¨ 0.5
2. **Gi·∫£m `iou_threshold`**: 0.4 thay v√¨ 0.5
3. **TƒÉng resolution**: Train/inference ·ªü 1280 thay v√¨ 640
4. **Multi-scale testing**: Inference nhi·ªÅu scales r·ªìi combine

```python
# Multi-scale inference
scales = [640, 800, 1024]
all_detections = []
for scale in scales:
    dets = inference(image, scale)
    all_detections.extend(dets)
final = apply_nms(all_detections)
```

### Q4: C√≥ th·ªÉ d√πng Soft-NMS thay v√¨ Hard-NMS kh√¥ng?

**A:** C√≥! Soft-NMS gi·∫£m score thay v√¨ lo·∫°i b·ªè ho√†n to√†n.

```python
def soft_nms(boxes, scores, iou_thresh=0.5, sigma=0.5):
    """
    Soft-NMS: Gi·∫£m score thay v√¨ remove
    """
    for i in range(len(boxes)):
        max_idx = scores.argmax()
        
        # T√≠nh IoU
        ious = calculate_iou(boxes[max_idx], boxes)
        
        # Decay scores thay v√¨ remove
        decay = np.exp(-(ious ** 2) / sigma)
        scores = scores * decay
        scores[max_idx] = -1  # ƒê√£ x·ª≠ l√Ω
    
    return keep_indices
```

**Soft-NMS t·ªët h∆°n cho:**
- Overlapping objects (people in crowd)
- Occlusion cases

### Q5: Output c·ªßa YOLO c√≥ th·ªÉ kh√°c 8400 kh√¥ng?

**A:** C√≥! Ph·ª• thu·ªôc v√†o ki·∫øn tr√∫c:

| Model | Input Size | Output Shape | S·ªë Detections |
|-------|-----------|--------------|---------------|
| YOLOv8n | 640 | (1, 84, 8400) | 8,400 |
| YOLOv8n | 1280 | (1, 84, 33600) | 33,600 |
| YOLOv5 | 640 | (1, 25200, 85) | 25,200 |

C√¥ng th·ª©c: `num_detections = Œ£(feature_map_h √ó feature_map_w)`

### Q6: L√†m sao ƒë·ªÉ deploy NMS l√™n JavaScript/C++?

**JavaScript (TensorFlow.js):**
```javascript
async function applyNMS(boxes, scores, iouThreshold) {
    const nmsIndices = await tf.image.nonMaxSuppressionAsync(
        boxes,
        scores,
        maxOutputSize,
        iouThreshold
    );
    return nmsIndices;
}
```

**C++ (OpenCV):**
```cpp
#include <opencv2/dnn.hpp>

std::vector<int> indices;
cv::dnn::NMSBoxes(
    boxes,           // std::vector<Rect>
    confidences,     // std::vector<float>
    conf_threshold,
    iou_threshold,
    indices
);
```

### Q7: T·∫°i sao c√≥ khi detections b·ªã "nh·∫•p nh√°y" (flickering) trong video?

**Nguy√™n nh√¢n:**
- Confidence score dao ƒë·ªông quanh threshold
- Boxes kh√¥ng ·ªïn ƒë·ªãnh gi·ªØa c√°c frames

**Gi·∫£i ph√°p:**
```python
# 1. Temporal smoothing
def smooth_detections(current_dets, prev_dets, alpha=0.7):
    """Smooth boxes gi·ªØa c√°c frames"""
    smoothed = alpha * current_dets + (1 - alpha) * prev_dets
    return smoothed

# 2. Tracking
# S·ª≠ d·ª•ng tracking algorithms (DeepSORT, ByteTrack)
tracker = DeepSORT()
tracked_objects = tracker.update(detections)
```

### Q8: Performance c·ªßa NMS trong real-time apps?

**Benchmarks (YOLOv8n, 640x640, CPU):**

| Stage | Time | % of Total |
|-------|------|-----------|
| Preprocessing | 2ms | 10% |
| Inference | 15ms | 70% |
| **NMS** | **4ms** | **20%** |
| **Total** | **21ms** | **100%** |

**K·∫øt lu·∫≠n:** NMS ch·ªâ chi·∫øm ~20% th·ªùi gian, kh√¥ng ph·∫£i bottleneck!

---

## 7. K·∫øt Lu·∫≠n

### üéØ Key Takeaways

1. **NMS l√† b·∫Øt bu·ªôc** khi deploy YOLO models sang c√°c format kh√°c
2. **Class-Aware NMS** t·ªët h∆°n Class-Agnostic cho general object detection
3. **Tuning thresholds** (`conf_threshold`, `iou_threshold`) r·∫•t quan tr·ªçng
4. **Vectorized operations** gi√∫p tƒÉng t·ªëc ƒë√°ng k·ªÉ
5. **Testing** tr√™n nhi·ªÅu scenarios ƒë·ªÉ t√¨m best settings

### üìö Ki·∫øn Th·ª©c B·∫°n ƒê√£ H·ªçc

- ‚úÖ Hi·ªÉu output th√¥ c·ªßa YOLO models (8400 √ó 84)
- ‚úÖ C√°ch t√≠nh IoU gi·ªØa 2 bounding boxes
- ‚úÖ Quy tr√¨nh 5 b∆∞·ªõc c·ªßa NMS post-processing
- ‚úÖ Implement NMS t·ª´ ƒë·∫ßu b·∫±ng NumPy
- ‚úÖ Best practices v√† common pitfalls
- ‚úÖ Tips ƒë·ªÉ optimize performance

### üöÄ Next Steps

1. **Practice**: Implement NMS cho custom datasets
2. **Experiment**: Test v·ªõi c√°c IoU thresholds kh√°c nhau
3. **Deploy**: Port code sang JavaScript/C++ cho production
4. **Advanced**: T√¨m hi·ªÉu Soft-NMS, DIoU-NMS
5. **Tracking**: Combine NMS v·ªõi object tracking algorithms

### üìñ T√†i Li·ªáu Tham Kh·∫£o

1. [Ultralytics YOLOv8 Official](https://github.com/ultralytics/ultralytics)
2. [YOLO Post-Processing Guide](https://dev.to/andreygermanov/how-to-create-yolov8-based-object-detection-web-service-using-python-julia-nodejs-javascript-go-and-rust-4o8e)
3. [Original NMS Paper](https://arxiv.org/abs/1704.04503)
4. [Soft-NMS Paper](https://arxiv.org/abs/1704.04503)
5. [ONNX Runtime Documentation](https://onnxruntime.ai/docs/)

---

## üôã‚Äç‚ôÇÔ∏è C√≥ Th·∫Øc M·∫Øc?

N·∫øu b·∫°n c√≥ c√¢u h·ªèi ho·∫∑c g·∫∑p v·∫•n ƒë·ªÅ khi implement NMS, feel free ƒë·ªÉ l·∫°i comment b√™n d∆∞·ªõi! M√¨nh s·∫Ω c·ªë g·∫Øng tr·∫£ l·ªùi s·ªõm nh·∫•t c√≥ th·ªÉ.

**Happy Coding!** üöÄ

---

*B√†i vi·∫øt ƒë∆∞·ª£c vi·∫øt b·ªüi Le Hoang Viet*


*C·∫≠p nh·∫≠t l·∫ßn cu·ªëi: 17th Feb 2026*
