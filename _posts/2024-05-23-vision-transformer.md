---
title: 'Vision Transformer (ViT) - Khi Transformer HoÌ£c CaÌch NhiÌ€n'
date: 2024-05-23
permalink: /posts/2024/05/vision_transformer/
tags:
  - vision transformer
  - computer vision
  - architecture 
---


BÃ i viáº¿t nÃ y sáº½ giáº£i thÃ­ch kiáº¿n trÃºc Vision Transformer tá»« lÃ½ thuyáº¿t Ä‘áº¿n code from scratch, giÃºp báº¡n hiá»ƒu táº¡i sao ViT láº¡i lÃ  má»™t bÆ°á»›c ngoáº·t lá»›n trong Computer Vision.

---

## Má»¥c Lá»¥c

1. [ViT Giáº£i Quyáº¿t Váº¥n Äá» GÃ¬?](#1-vit-giáº£i-quyáº¿t-váº¥n-Ä‘á»-gÃ¬)
2. [Patch â€” Biáº¿n áº¢nh ThÃ nh "Tá»«"](#2-patch--biáº¿n-áº£nh-thÃ nh-tá»«)
3. [Kiáº¿n TrÃºc Chi Tiáº¿t](#3-kiáº¿n-trÃºc-chi-tiáº¿t)
4. [Implement ViT Tá»« Äáº§u](#4-implement-vit-tá»«-Ä‘áº§u)
5. [Training TrÃªn CIFAR-10](#5-training-trÃªn-cifar-10)
6. [Tips & Best Practices](#6-tips--best-practices)
7. [FAQs - CÃ¢u Há»i ThÆ°á»ng Gáº·p](#7-faq---cÃ¢u-há»i-thÆ°á»ng-gáº·p)

---

## 1. ViT Giáº£i Quyáº¿t Váº¥n Äá» GÃ¬?

Sau khi Transformer ra máº¯t vÃ o nÄƒm **2017** (trong paper *"Attention Is All You Need"*), nÃ³ Ä‘Ã£ táº¡o ra má»™t cuá»™c cÃ¡ch máº¡ng trong NLP â€” thay tháº¿ hoÃ n toÃ n cÃ¡c kiáº¿n trÃºc RNN/LSTM vá»‘n cháº­m vÃ  khÃ³ song song hÃ³a.

Tuy nhiÃªn, cÃ¢u há»i Ä‘áº·t ra lÃ : **Transformer cÃ³ thá»ƒ dÃ¹ng cho áº£nh khÃ´ng?**

Váº¥n Ä‘á» cá»‘t lÃµi lÃ  Transformer Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ **chuá»—i (sequence)** â€” má»™t dÃ£y cÃ¡c token rá»i ráº¡c. Trong khi Ä‘Ã³, áº£nh lÃ  má»™t **lÆ°á»›i 2D pixel liÃªn tá»¥c**, hoÃ n toÃ n khÃ¡c vá» báº£n cháº¥t. Äá»ƒ dÃ¹ng Transformer tháº³ng cho áº£nh, báº¡n sáº½ pháº£i flatten toÃ n bá»™ pixel â€” vá»›i áº£nh 224Ã—224, Ä‘Ã³ lÃ  **50,176 token** cho má»—i táº¥m hÃ¬nh, bá»™ nhá»› sáº½ bÃ¹ng ná»• ngay láº­p tá»©c.

**Alexey Dosovitskiy et al.** Ä‘Ã£ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y trong paper ná»•i tiáº¿ng:

> *"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"* (2020)

Ã tÆ°á»Ÿng cá»‘t lÃµi ráº¥t Ä‘Æ¡n giáº£n: **Chia táº¥m áº£nh thÃ nh cÃ¡c máº£nh nhá» (patches), má»—i patch tÆ°Æ¡ng Ä‘Æ°Æ¡ng má»™t "tá»«" trong cÃ¢u.** Báº±ng cÃ¡ch Ä‘Ã³, má»™t áº£nh 224Ã—224 vá»›i patch size 16Ã—16 chá»‰ cÃ²n **(224/16)Â² = 196 token** â€” giáº£m hÆ¡n 250 láº§n.

---

## 2. Patch â€” Biáº¿n áº¢nh ThÃ nh "Tá»«"

### 2.1 PhÃ©p Biáº¿n Äá»•i Cá»‘t LÃµi

ToÃ n bá»™ sá»± khÃ¡c biá»‡t giá»¯a ViT vÃ  Transformer gá»‘c náº±m á»Ÿ bÆ°á»›c tiá»n xá»­ lÃ½ input nÃ y:

```
áº¢nh gá»‘c:    (H, W, C)
              â†“  chia thÃ nh cÃ¡c patch
Patch:       (N, PÃ—PÃ—C)
```

Trong Ä‘Ã³:
- `H, W` â€” chiá»u cao vÃ  chiá»u rá»™ng cá»§a áº£nh
- `C` â€” sá»‘ channel (thÆ°á»ng lÃ  3 cho RGB)
- `P` â€” patch size (thÆ°á»ng lÃ  16)
- `N = (H/P) Ã— (W/P)` â€” sá»‘ lÆ°á»£ng patches (sá»‘ "tá»«" trong cÃ¢u)

VÃ­ dá»¥ vá»›i áº£nh ImageNet chuáº©n (224Ã—224Ã—3) vÃ  patch size 16:
- Sá»‘ patch: `(224/16) Ã— (224/16) = 14 Ã— 14 = 196 patches`
- Má»—i patch cÃ³ kÃ­ch thÆ°á»›c: `16 Ã— 16 Ã— 3 = 768 chiá»u`

### 2.2 Class Token & Positional Encoding

Sau khi cÃ³ cÃ¡c patches, ViT thÃªm hai thá»© quan trá»ng:

**Class Token `[CLS]`**: Má»™t token Ä‘áº·c biá»‡t Ä‘Æ°á»£c thÃªm vÃ o Ä‘áº§u chuá»—i, há»c cÃ¡ch tá»•ng há»£p thÃ´ng tin tá»« toÃ n bá»™ áº£nh. Output cá»§a token nÃ y sáº½ Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ phÃ¢n loáº¡i (tÆ°Æ¡ng tá»± `[CLS]` trong BERT).

**Positional Embedding**: Transformer khÃ´ng cÃ³ khÃ¡i niá»‡m thá»© tá»±, nÃªn ta pháº£i cá»™ng thÃªm thÃ´ng tin vá»‹ trÃ­ vÃ o má»—i patch embedding Ä‘á»ƒ mÃ´ hÃ¬nh biáº¿t patch nÃ y náº±m á»Ÿ Ä‘Ã¢u trong áº£nh.

![ViT Architecture](/images/vit_architecture.png)
*HÃ¬nh: Kiáº¿n trÃºc Vision Transformer â€” áº£nh Ä‘Æ°á»£c chia thÃ nh patches, qua embedding rá»“i Ä‘Æ°a vÃ o Transformer encoder*

---

## 3. Kiáº¿n TrÃºc Chi Tiáº¿t

ViT gá»“m 3 pháº§n chÃ­nh:

```
Input Image
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Patch Extractor â”‚  â† Chia áº£nh thÃ nh N patches
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding Layer â”‚  â† Linear projection + Positional Encoding
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Transformer Encoder        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Multi-Head Attention  â”‚  â”‚
â”‚  â”‚  + Layer Norm          â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  Feed Forward Block    â”‚  â”‚
â”‚  â”‚  + Layer Norm          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚        Ã— L layers            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MLP Head       â”‚  â† Output class probabilities
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Sau khi qua Transformer, ta flatten output vÃ  Ä‘Æ°a qua má»™t lá»›p linear Ä‘á»ƒ dá»± Ä‘oÃ¡n class. Trong báº£n gá»‘c cá»§a paper, chá»‰ cÃ³ class token Ä‘Æ°á»£c dÃ¹ng cho classification â€” nhÆ°ng trong implement Ä‘Æ¡n giáº£n hÃ³a dÆ°á»›i Ä‘Ã¢y, ta sáº½ flatten toÃ n bá»™ output.

---

## 4. Implement ViT Tá»« Äáº§u

### 4.1 Setup

```python
import torch
import torch.nn as nn
import numpy as np
```

### 4.2 Patch Extractor

Module nÃ y nháº­n vÃ o má»™t batch áº£nh vÃ  tráº£ vá» cÃ¡c patches dÆ°á»›i dáº¡ng chuá»—i:

```python
class PatchExtractor(nn.Module):
    def __init__(self, patch_size=16):
        super().__init__()
        self.patch_size = patch_size

    def forward(self, x):
        # x shape: (batch_size, channels, height, width)
        B, C, H, W = x.size()
        
        assert H % self.patch_size == 0 and W % self.patch_size == 0, \
            f"Image size ({H}x{W}) pháº£i chia háº¿t cho patch_size ({self.patch_size})"

        num_patches_h = H // self.patch_size
        num_patches_w = W // self.patch_size
        num_patches = num_patches_h * num_patches_w

        # unfold: trÃ­ch xuáº¥t sliding windows theo chiá»u H vÃ  W
        patches = (
            x.unfold(2, self.patch_size, self.patch_size)
             .unfold(3, self.patch_size, self.patch_size)
             .permute(0, 2, 3, 1, 4, 5)
             .contiguous()
             .view(B, num_patches, -1)
        )
        # Output shape: (batch_size, num_patches, patch_size*patch_size*channels)
        return patches
```

**Giáº£i thÃ­ch tá»«ng bÆ°á»›c:**
- `unfold(2, P, P)` â€” trÆ°á»£t cá»­a sá»• `PÃ—P` theo chiá»u H
- `unfold(3, P, P)` â€” trÆ°á»£t theo chiá»u W
- `permute(0, 2, 3, 1, 4, 5)` â€” sáº¯p xáº¿p láº¡i thÃ nh `(B, nH, nW, C, P, P)`
- `view(B, num_patches, -1)` â€” flatten má»—i patch thÃ nh vector 1D

### 4.3 Embedding Layer

Chiáº¿u má»—i patch lÃªn khÃ´ng gian `latent_size` chiá»u vÃ  cá»™ng thÃªm positional encoding:

```python
class EmbeddingLayer(nn.Module):
    def __init__(self, latent_size=1024, num_patches=4, input_dim=768):
        super().__init__()
        self.num_patches = num_patches
        
        # Chiáº¿u patch thÃ´ â†’ latent space
        self.input_embedder = nn.Linear(input_dim, latent_size)
        
        # Há»c positional encoding tá»« chá»‰ sá»‘ vá»‹ trÃ­
        self.pos_embedder = nn.Linear(1, latent_size)
        
        # Táº¡o chá»‰ sá»‘ vá»‹ trÃ­: [0, 1, 2, ..., num_patches-1]
        self.register_buffer(
            'positional_information',
            torch.arange(0, num_patches).reshape(1, num_patches, 1).float()
        )

    def forward(self, x):
        # x shape: (N, num_patches, input_dim)
        N = x.shape[0]
        
        input_embedding = self.input_embedder(x)
        
        pos_info = self.positional_information.expand(N, -1, -1)
        positional_embedding = self.pos_embedder(pos_info)
        
        # Cá»™ng patch embedding vÃ  positional embedding
        return input_embedding + positional_embedding
```

> ğŸ’¡ **LÆ°u Ã½**: á» Ä‘Ã¢y ta dÃ¹ng `register_buffer` thay vÃ¬ `self.positional_information` Ä‘á»ƒ `positional_information` tá»± Ä‘á»™ng chuyá»ƒn sang Ä‘Ãºng device (CPU/GPU) cÃ¹ng vá»›i model.

### 4.4 Model ViT

```python
class ViT(nn.Module):
    def __init__(
        self,
        patch_size: int = 16,
        img_dimension: tuple = (32, 32),
        latent_size: int = 1024,
        num_heads: int = 2,
        num_classes: int = 10,
        dropout: float = 0.1,
    ):
        super().__init__()
        
        H, W = img_dimension
        assert H % patch_size == 0 and W % patch_size == 0, \
            "KÃ­ch thÆ°á»›c áº£nh pháº£i chia háº¿t cho patch_size!"

        self.num_patches = (H // patch_size) * (W // patch_size)
        input_dim = patch_size * patch_size * 3  # 3 channels RGB

        # --- CÃ¡c thÃ nh pháº§n chÃ­nh ---
        self.patchifier = PatchExtractor(patch_size)
        
        self.embedding_layer = EmbeddingLayer(
            latent_size=latent_size,
            num_patches=self.num_patches,
            input_dim=input_dim,
        )
        
        # Multi-Head Self-Attention
        self.multi_head_attn = nn.MultiheadAttention(
            embed_dim=latent_size,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True,  # Input: (batch, seq, feature)
        )
        
        self.norm_1 = nn.LayerNorm(latent_size)
        self.norm_2 = nn.LayerNorm(latent_size)
        self.dropout = nn.Dropout(dropout)

        # Feed Forward Block: má»Ÿ rá»™ng â†’ thu háº¹p
        self.feed_forward = nn.Sequential(
            nn.Linear(latent_size, latent_size * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(latent_size * 4, latent_size),
        )
        
        # Classification head
        self.output_layer = nn.Linear(latent_size * self.num_patches, num_classes)

    def forward(self, x):
        # BÆ°á»›c 1: TÃ¡ch áº£nh thÃ nh patches
        x = self.patchifier(x)                          # (B, N, P*P*C)
        
        # BÆ°á»›c 2: Embedding + Positional Encoding
        x = self.embedding_layer(x)                     # (B, N, latent_size)
        
        # BÆ°á»›c 3: Multi-Head Self-Attention vá»›i residual connection
        attn_out, _ = self.multi_head_attn(x, x, x)
        x = self.norm_1(self.dropout(attn_out) + x)     # Pre-norm + residual
        
        # BÆ°á»›c 4: Feed Forward vá»›i residual connection
        x = self.norm_2(self.feed_forward(x) + x)       # (B, N, latent_size)
        
        # BÆ°á»›c 5: Flatten vÃ  phÃ¢n loáº¡i
        x = x.flatten(start_dim=1)                      # (B, N*latent_size)
        x = self.output_layer(x)                         # (B, num_classes)
        return x
```

**CÃ¡c cáº£i tiáº¿n so vá»›i phiÃªn báº£n gá»‘c:**
- ThÃªm `batch_first=True` cho `MultiheadAttention` â€” input Ä‘Ãºng format `(B, N, D)`
- DÃ¹ng `GELU` thay vÃ¬ `ReLU` trong FFN (Ä‘Ãºng vá»›i paper gá»‘c)
- Dropout regularization Ä‘á»ƒ giáº£m overfitting
- `register_buffer` cho positional info Ä‘á»ƒ tá»± Ä‘á»™ng chuyá»ƒn device

---

## 5. Training TrÃªn CIFAR-10

```python
import torchvision
import torchvision.transforms as transforms
from torch import optim
from tqdm import tqdm

# --- Data ---
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

BATCH_SIZE = 64

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
testset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
testloader  = torch.utils.data.DataLoader(testset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

CLASSES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

# --- Model ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = ViT(
    patch_size=8,           # 32/8 = 4 patches má»—i chiá»u â†’ 16 patches tá»•ng
    img_dimension=(32, 32),
    latent_size=512,
    num_heads=4,
    num_classes=10,
).to(device)

print(f"Sá»‘ tham sá»‘: {sum(p.numel() for p in model.parameters()):,}")

# --- Training ---
optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)
loss_fn   = nn.CrossEntropyLoss()

def train_epoch(loader):
    model.train()
    total_loss, correct, total = 0, 0, 0
    for x, y in tqdm(loader, desc="Training"):
        x, y = x.to(device), y.to(device)
        logits = model(x)
        loss = loss_fn(logits, y)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        correct += (logits.argmax(1) == y).sum().item()
        total += len(y)
    
    return total_loss / len(loader), correct / total

@torch.no_grad()
def evaluate(loader):
    model.eval()
    total_loss, correct, total = 0, 0, 0
    for x, y in tqdm(loader, desc="Evaluating"):
        x, y = x.to(device), y.to(device)
        logits = model(x)
        total_loss += loss_fn(logits, y).item()
        correct += (logits.argmax(1) == y).sum().item()
        total += len(y)
    
    return total_loss / len(loader), correct / total

def train(epochs=20):
    for epoch in range(epochs):
        train_loss, train_acc = train_epoch(trainloader)
        val_loss, val_acc     = evaluate(testloader)
        scheduler.step()
        
        print(f"Epoch {epoch+1:02d}/{epochs} | "
              f"Train Loss: {train_loss:.4f} Acc: {train_acc:.3f} | "
              f"Val Loss: {val_loss:.4f} Acc: {val_acc:.3f}")

train(20)
```

> **Káº¿t quáº£ tham kháº£o**: Vá»›i thiáº¿t láº­p trÃªn, sau ~20 epochs, báº¡n cÃ³ thá»ƒ Ä‘áº¡t khoáº£ng **60-65% accuracy** trÃªn CIFAR-10. ViT nhá» khÃ´ng giá»i trÃªn dataset nhá» â€” Ä‘Ã¢y lÃ  Ä‘iá»ƒm yáº¿u ná»•i tiáº¿ng cá»§a kiáº¿n trÃºc nÃ y (xem thÃªm á»Ÿ pháº§n FAQ).

---

## 6. Tips & Best Practices

**Chá»n patch size phÃ¹ há»£p**
Patch size pháº£i lÃ  Æ°á»›c sá»‘ cá»§a cáº£ H vÃ  W. Patch nhá» hÆ¡n â†’ nhiá»u token hÆ¡n â†’ context phong phÃº hÆ¡n nhÆ°ng tá»‘n bá»™ nhá»› hÆ¡n. Vá»›i áº£nh 32Ã—32, dÃ¹ng `patch_size=4` hoáº·c `8`. Vá»›i áº£nh 224Ã—224, dÃ¹ng `patch_size=16` hoáº·c `32`.

**ViT cáº§n nhiá»u dá»¯ liá»‡u**
ViT thiáº¿u inductive bias vá» cáº¥u trÃºc cá»¥c bá»™ (nhÆ° CNN), nÃªn cáº§n dataset lá»›n (Ã­t nháº¥t vÃ i trÄƒm nghÃ¬n áº£nh) Ä‘á»ƒ há»™i tá»¥ tá»‘t. Vá»›i dataset nhá» nhÆ° CIFAR-10, hÃ£y dÃ¹ng **pretrained ViT** vÃ  fine-tune.

**DÃ¹ng pretrained model khi cÃ³ thá»ƒ**
```python
# Thay vÃ¬ train from scratch, dÃ¹ng torchvision hoáº·c timm
import timm
model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=10)
```

**Learning rate vÃ  warmup**
Transformer ráº¥t nháº¡y vá»›i learning rate. NÃªn dÃ¹ng learning rate warmup + cosine decay, vÃ  `AdamW` thay vÃ¬ `Adam`.

**Regularization**
ThÃªm `Dropout` (0.1-0.2) vÃ  `weight_decay` trong optimizer Ä‘á»ƒ trÃ¡nh overfitting, Ä‘áº·c biá»‡t khi train tá»« Ä‘áº§u.

---

## 7. FAQ - CÃ¢u Há»i ThÆ°á»ng Gáº·p

**â“ Táº¡i sao ViT kÃ©m hÆ¡n CNN trÃªn dataset nhá»?**

CNN cÃ³ sáºµn hai "thiÃªn kiáº¿n quy náº¡p" (inductive bias): *locality* (pixel gáº§n nhau thÆ°á»ng liÃªn quan) vÃ  *translation equivariance* (object á»Ÿ Ä‘Ã¢u cÅ©ng nháº­n ra). ViT khÃ´ng cÃ³ nhá»¯ng giáº£ Ä‘á»‹nh nÃ y â€” nÃ³ há»c chÃºng tá»« dá»¯ liá»‡u. VÃ¬ váº­y ViT cáº§n nhiá»u dá»¯ liá»‡u hÆ¡n Ä‘á»ƒ bÃ¹ láº¡i, nhÆ°ng khi cÃ³ Ä‘á»§ data nÃ³ thÆ°á»ng vÆ°á»£t trá»™i CNN.

**â“ Class token `[CLS]` lÃ  gÃ¬ vÃ  cÃ³ báº¯t buá»™c khÃ´ng?**

`[CLS]` lÃ  má»™t vector há»c Ä‘Æ°á»£c, Ä‘áº·t á»Ÿ Ä‘áº§u chuá»—i. Sau khi qua Transformer, nÃ³ sáº½ "attend" Ä‘áº¿n táº¥t cáº£ patches vÃ  tá»•ng há»£p thÃ´ng tin toÃ n áº£nh. Trong paper gá»‘c, chá»‰ output cá»§a `[CLS]` token Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ phÃ¢n loáº¡i. KhÃ´ng báº¯t buá»™c â€” má»™t sá»‘ variant dÃ¹ng average pooling trÃªn táº¥t cáº£ token thay tháº¿.

**â“ Positional Encoding há»c Ä‘Æ°á»£c hay cá»‘ Ä‘á»‹nh?**

Paper gá»‘c dÃ¹ng positional encoding **há»c Ä‘Æ°á»£c** (learned), khÃ´ng pháº£i sinusoidal cá»‘ Ä‘á»‹nh. ThÃº vá»‹ lÃ  káº¿t quáº£ khÃ´ng khÃ¡c nhau nhiá»u giá»¯a hai cÃ¡ch, nhÆ°ng learned encoding thÆ°á»ng linh hoáº¡t hÆ¡n.

**â“ ViT cÃ³ thá»ƒ xá»­ lÃ½ áº£nh kÃ­ch thÆ°á»›c khÃ¡c nhau khÃ´ng?**

KhÃ´ng dá»… dÃ ng. VÃ¬ positional encoding cá»‘ Ä‘á»‹nh theo sá»‘ patches, náº¿u áº£nh thay Ä‘á»•i kÃ­ch thÆ°á»›c thÃ¬ sá»‘ patches thay Ä‘á»•i vÃ  encoding khÃ´ng khá»›p ná»¯a. Má»™t sá»‘ giáº£i phÃ¡p: interpolate positional encoding, dÃ¹ng RoPE (Rotary Positional Embedding), hoáº·c resize áº£nh vá» kÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh trÆ°á»›c.

**â“ Sá»± khÃ¡c biá»‡t giá»¯a ViT-B, ViT-L, ViT-H lÃ  gÃ¬?**

ÄÃ¢y lÃ  cÃ¡c biáº¿n thá»ƒ kÃ­ch thÆ°á»›c khÃ¡c nhau trong paper gá»‘c:

| Model   | Layers | Hidden size D | MLP size | Heads | Params |
|---------|--------|---------------|----------|-------|--------|
| ViT-B   | 12     | 768           | 3072     | 12    | 86M    |
| ViT-L   | 24     | 1024          | 4096     | 16    | 307M   |
| ViT-H   | 32     | 1280          | 5120     | 16    | 632M   |

---

## Káº¿t

Trong bÃ i viáº¿t nÃ y, mÃ¬nh Ä‘Ã£ giá»›i thiá»‡u Vision Transformer tá»« váº¥n Ä‘á» nÃ³ giáº£i quyáº¿t, Ä‘i sÃ¢u vÃ o tá»«ng thÃ nh pháº§n kiáº¿n trÃºc, vÃ  implement hoÃ n chá»‰nh from scratch. Äiá»ƒm máº¥u chá»‘t cáº§n nhá»›: **ViT chá»‰ lÃ  Transformer gá»‘c, vá»›i bÆ°á»›c tiá»n xá»­ lÃ½ Ä‘áº·c biá»‡t biáº¿n áº£nh thÃ nh chuá»—i patch.**

á» cÃ¡c bÃ i tiáº¿p theo, mÃ¬nh sáº½ giá»›i thiá»‡u cÃ¡c kiáº¿n trÃºc transformer-based khÃ¡c cho cÃ¡c task phá»©c táº¡p hÆ¡n nhÆ° **image segmentation (Segmenter, Mask2Former)** vÃ  **object detection (DETR, DINO)**. ChÃºc cÃ¡c báº¡n há»c tá»‘t! ğŸš€

---

## References

1. [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) â€” Dosovitskiy et al., 2020
2. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) â€” Vaswani et al., 2017
3. [Training data-efficient image transformers & distillation through attention (DeiT)](https://arxiv.org/abs/2012.12877) â€” Touvron et al., 2021
